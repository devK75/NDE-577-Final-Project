{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8v3i9mRdzCs"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy import genfromtxt\n",
        "import pandas as pd\n",
        "import io"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The vanishing gradient problem happens when the gradient becomes too small and gets very close to zero, that does not allow the network to converge properly. Typically happens when you multiply together several derivatives that are already small, using the chain rule and it is very common in RNN. Typically, a way to solve vanishing gradient would be to use relu/leaky relu  activation functions, using gradient clipping or use LSTM. LSTM is probably the best choice since it use a unique additive gradient structure controlled by the forget gate and the activation function used works as an identity function allowing the gradient not to vanish durning the backpropagation. For example since a RNN is traind using back propagation through layers and through time, in each time step we need to sum up all the previous contribution untill the current one. When their derivatives become smaller than one and they're multipled together, the gradient approaches zero and we have the vanishing gradient problem."
      ],
      "metadata": {
        "id": "0SD4Zj8cpBCY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Typically the best data for RNN/LSTM is sequence data like time-series or text data."
      ],
      "metadata": {
        "id": "0CFEPhOJpCj_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reddit = pd.read_csv(\"reddit.csv\")"
      ],
      "metadata": {
        "id": "j5_loRAgkuCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reddit.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "iG-dVQzlk8tv",
        "outputId": "44703e43-1092-4dcb-d53e-4976b6683fdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                body\n",
              "0  I joined a new league this year and they have ...\n",
              "1  In your scenario, a person could just not run ...\n",
              "2  They don't get paid for how much time you spen...\n",
              "3  I dunno, back before the August update in an A...\n",
              "4  No, but Toriyama sometimes would draw himself ..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e079a5d1-13bf-4435-b135-88cbebf82334\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>body</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I joined a new league this year and they have ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>In your scenario, a person could just not run ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>They don't get paid for how much time you spen...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I dunno, back before the August update in an A...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>No, but Toriyama sometimes would draw himself ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e079a5d1-13bf-4435-b135-88cbebf82334')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e079a5d1-13bf-4435-b135-88cbebf82334 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e079a5d1-13bf-4435-b135-88cbebf82334');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xbCNMisbjV7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "import itertools\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "def getSentenceData(path, vocabulary_size=8000):\n",
        "    unknown_token = \"UNKNOWN_TOKEN\"\n",
        "    sentence_start_token = \"SENTENCE_START\"\n",
        "    sentence_end_token = \"SENTENCE_END\"\n",
        "\n",
        "    # Read the data and append SENTENCE_START and SENTENCE_END tokens\n",
        "    print(\"Reading CSV file...\")\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        reader = csv.reader(f, skipinitialspace=True)\n",
        "        # Split full comments into sentences\n",
        "        sentences = itertools.chain(*[nltk.sent_tokenize(x[0].lower()) for x in reader])\n",
        "        # Append SENTENCE_START and SENTENCE_END\n",
        "        sentences = [\"%s %s %s\" % (sentence_start_token, x, sentence_end_token) for x in sentences]\n",
        "    print(\"Parsed %d sentences.\" % (len(sentences)))\n",
        "\n",
        "    # Tokenize the sentences into words\n",
        "    tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
        "    # Filter the sentences having few words (including SENTENCE_START and SENTENCE_END)\n",
        "    tokenized_sentences = list(filter(lambda x: len(x) > 3, tokenized_sentences))\n",
        "\n",
        "    # Count the word frequencies\n",
        "    word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
        "    print(\"Found %d unique words tokens.\" % len(word_freq.items()))\n",
        "\n",
        "    # Get the most common words and build index_to_word and word_to_index vectors\n",
        "    vocab = word_freq.most_common(vocabulary_size-1)\n",
        "    index_to_word = [x[0] for x in vocab]\n",
        "    index_to_word.append(unknown_token)\n",
        "    word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])\n",
        "\n",
        "    print(\"Using vocabulary size %d.\" % vocabulary_size)\n",
        "    print(\"The least frequent word in our vocabulary is '%s' and appeared %d times.\" % (vocab[-1][0], vocab[-1][1]))\n",
        "\n",
        "    # Replace all words not in our vocabulary with the unknown token\n",
        "    for i, sent in enumerate(tokenized_sentences):\n",
        "        tokenized_sentences[i] = [w if w in word_to_index else unknown_token for w in sent]\n",
        "\n",
        "    print(\"\\nExample sentence: '%s'\" % sentences[1])\n",
        "    print(\"\\nExample sentence after Pre-processing: '%s'\\n\" % tokenized_sentences[0])\n",
        "\n",
        "    # Create the training data\n",
        "    X_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences])\n",
        "    y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences])\n",
        "\n",
        "    print(\"X_train shape: \" + str(X_train.shape))\n",
        "    print(\"y_train shape: \" + str(y_train.shape))\n",
        "\n",
        "    # Print an training data example\n",
        "    x_example, y_example = X_train[17], y_train[17]\n",
        "    print(\"x:\\n%s\\n%s\" % (\" \".join([index_to_word[x] for x in x_example]), x_example))\n",
        "    print(\"\\ny:\\n%s\\n%s\" % (\" \".join([index_to_word[x] for x in y_example]), y_example))\n",
        "\n",
        "    return X_train, y_train\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    X_train, y_train = getSentenceData('reddit.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6ZUhLNki-TG",
        "outputId": "6c3430ea-b1a5-4e87-fd1b-cd3df4c596d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading CSV file...\n",
            "Parsed 79062 sentences.\n",
            "Found 63006 unique words tokens.\n",
            "Using vocabulary size 8000.\n",
            "The least frequent word in our vocabulary is 'appointments' and appeared 10 times.\n",
            "\n",
            "Example sentence: 'SENTENCE_START i joined a new league this year and they have different scoring rules than i'm used to. SENTENCE_END'\n",
            "\n",
            "Example sentence after Pre-processing: '['SENTENCE_START', 'i', 'joined', 'a', 'new', 'league', 'this', 'year', 'and', 'they', 'have', 'different', 'scoring', 'rules', 'than', 'i', \"'m\", 'used', 'to', '.', 'SENTENCE_END']'\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (78489,)\n",
            "y_train shape: (78489,)\n",
            "x:\n",
            "SENTENCE_START what are n't you understanding about this ? !\n",
            "[0, 52, 28, 17, 10, 858, 55, 26, 35, 70]\n",
            "\n",
            "y:\n",
            "what are n't you understanding about this ? ! SENTENCE_END\n",
            "[52, 28, 17, 10, 858, 55, 26, 35, 70, 1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:49: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class RNNLayer:\n",
        "    def forward(self, x, prev_s, U, W, V):\n",
        "        self.mulu = mulGate.forward(U, x)\n",
        "        self.mulw = mulGate.forward(W, prev_s)\n",
        "        self.add = addGate.forward(self.mulw, self.mulu)\n",
        "        #state output\n",
        "        self.s = activation.forward(self.add)\n",
        "        self.mulv = mulGate.forward(V, self.s)\n",
        "\n",
        "    def backward(self, x, prev_s, U, W, V, diff_s, dmulv):\n",
        "        self.forward(x, prev_s, U, W, V)\n",
        "        dV, dsv = mulGate.backward(V, self.s, dmulv)\n",
        "        ds = dsv + diff_s\n",
        "        dadd = activation.backward(self.add, ds)\n",
        "        dmulw, dmulu = addGate.backward(self.mulw, self.mulu, dadd)\n",
        "        dW, dprev_s = mulGate.backward(W, prev_s, dmulw)\n",
        "        dU, dx = mulGate.backward(U, x, dmulu)\n",
        "        return (dprev_s, dU, dW, dV)\n",
        "\n"
      ],
      "metadata": {
        "id": "BrU8bQCplIgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class MultiplyGate:\n",
        "    def forward(self,W, x):\n",
        "        return np.dot(W, x)\n",
        "\n",
        "    def backward(self, W, x, dz):\n",
        "        dW = np.asarray(np.dot(np.transpose(np.asmatrix(dz)), np.asmatrix(x)))\n",
        "        dx = np.dot(np.transpose(W), dz)\n",
        "        return dW, dx\n",
        "\n",
        "class AddGate:\n",
        "    def forward(self, x1, x2):\n",
        "        return x1 + x2\n",
        "\n",
        "    def backward(self, x1, x2, dz):\n",
        "        dx1 = dz * np.ones_like(x1)\n",
        "        dx2 = dz * np.ones_like(x2)\n",
        "        return dx1, dx2"
      ],
      "metadata": {
        "id": "4IB5hFialJfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Sigmoid:\n",
        "    def forward(self, x):\n",
        "        return 1.0 / (1.0 + np.exp(-x))\n",
        "\n",
        "    def backward(self, x, top_diff):\n",
        "        output = self.forward(x)\n",
        "        return (1.0 - output) * output * top_diff\n",
        "\n",
        "class Tanh:\n",
        "    def forward(self, x):\n",
        "        return np.tanh(x)\n",
        "\n",
        "    def backward(self, x, top_diff):\n",
        "        output = self.forward(x)\n",
        "        return (1 - np.square(output)) * top_diff"
      ],
      "metadata": {
        "id": "9j-xyQpHoILV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Softmax:\n",
        "    def predict(self, x):\n",
        "        exp_scores = np.exp(x)\n",
        "        return exp_scores / np.sum(exp_scores)\n",
        "\n",
        "    def loss(self, x, y):\n",
        "        probs = self.predict(x)\n",
        "        return -np.log(probs[y])\n",
        "\n",
        "    def diff(self, x, y):\n",
        "        probs = self.predict(x)\n",
        "        probs[y] -= 1.0\n",
        "        return probs"
      ],
      "metadata": {
        "id": "nCi1-KIQqEk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "\n",
        "\n",
        "class Model:\n",
        "    def __init__(self, word_dim, hidden_dim=100, bptt_truncate=4):\n",
        "        self.word_dim = word_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.bptt_truncate = bptt_truncate\n",
        "        #initialize U,V,W in the recommended interval\n",
        "        self.U = np.random.uniform(-1. / np.sqrt(word_dim), 1. / np.sqrt(word_dim), (hidden_dim, word_dim))\n",
        "        self.W = np.random.uniform(-1. / np.sqrt(hidden_dim), 1. / np.sqrt(hidden_dim), (hidden_dim, hidden_dim))\n",
        "        self.V = np.random.uniform(-1. / np.sqrt(hidden_dim), 1. / np.sqrt(hidden_dim), (word_dim, hidden_dim))\n",
        "    '''\n",
        "        forward propagation (predicting word probabilities)\n",
        "        x is one single data, and a batch of data\n",
        "        for example x = [0, 179, 341, 416], then its y = [179, 341, 416, 1]\n",
        "    '''\n",
        "    def forward_propagation(self, x):\n",
        "        # The total number of time steps\n",
        "        n = len(x)\n",
        "        layers = []\n",
        "        prev_s = np.zeros(self.hidden_dim)\n",
        "        # For each time step...\n",
        "        for i in range(n):\n",
        "            # create an instance of our RNNLayer\n",
        "            rnn_layer = RNNLayer()\n",
        "            input = np.zeros(self.word_dim)\n",
        "            input[x[i]] = 1\n",
        "            rnn_layer.forward(input, prev_s, self.U, self.W, self.V)\n",
        "            # call forward for our layer here.\n",
        "            prev_s = rnn_layer.s\n",
        "            layers.append(rnn_layer)\n",
        "        return layers\n",
        "\n",
        "    def predict(self, x):\n",
        "        output = Softmax()\n",
        "        layers = self.forward_propagation(x)\n",
        "        #get the index of the biggest element in each layer output vector\n",
        "        return [np.argmax(output.predict(layer.mulv)) for layer in layers]\n",
        "\n",
        "    def calculate_loss(self, x, y):\n",
        "        output = Softmax()\n",
        "        layers = self.forward_propagation(x)\n",
        "        loss = 0.0\n",
        "        for i, layer in enumerate(layers):\n",
        "          loss += output.loss(layer.mulv, y[i])\n",
        "        return loss / float(len(y))\n",
        "\n",
        "\n",
        "    def calculate_total_loss(self, X, Y):\n",
        "        loss = 0.0\n",
        "        for i in range(len(Y)):\n",
        "            loss += self.calculate_loss(X[i], Y[i])\n",
        "        return loss / float(len(Y))\n",
        "\n",
        "    def bptt(self, x, y):\n",
        "        output = Softmax()\n",
        "        layers = self.forward_propagation(x)\n",
        "        #initialize to zero the network parameters\n",
        "        der_U = np.zeros(self.U.shape)\n",
        "        der_V = np.zeros(self.V.shape)\n",
        "        der_W = np.zeros(self.W.shape)\n",
        "\n",
        "        n_layers = len(layers)\n",
        "        prev_status_i = np.zeros(self.hidden_dim)\n",
        "        diff_status = np.zeros(self.hidden_dim)\n",
        "        for i in range(0, n_layers):\n",
        "            #derivative Mulv gate\n",
        "            der_mulv = output.diff(layers[i].mulv, y[i])\n",
        "            input = np.zeros(self.word_dim)\n",
        "            input[x[i]] = 1\n",
        "            #backpropagation step for each layer, calculate derivative of previous status, U,V,W\n",
        "            der_prev_status, der_U_i, der_W_i, der_V_i = layers[i].backward(input, prev_status_i, self.U, self.W, self.V, diff_status, der_mulv)\n",
        "            #Update the previous status\n",
        "            prev_status_i = layers[i].s\n",
        "            der_mulv = np.zeros(self.word_dim)\n",
        "            #backpropagation through time in the opposite direction\n",
        "            for j in range(i-1, max(-1, i-self.bptt_truncate-1), -1):\n",
        "                input = np.zeros(self.word_dim)\n",
        "                input[x[j]] = 1\n",
        "                prev_status_j = np.zeros(self.hidden_dim) if j == 0 else layers[j-1].s\n",
        "                #backpropagation step for each layer, calculate derivative of previous status, U,V,W\n",
        "                der_prev_status, der_U_j, der_W_j, der_V_j = layers[j].backward(input, prev_status_j, self.U, self.W, self.V, der_prev_status, der_mulv)\n",
        "                der_U_i += der_U_j\n",
        "                der_W_i += der_W_j\n",
        "            der_V += der_V_i\n",
        "            der_U += der_U_i\n",
        "            der_W += der_W_i\n",
        "        return (der_U, der_W, der_V)\n",
        "\n",
        "    def sgd_step(self, x, y, learning_rate):\n",
        "        dU, dW, dV = self.bptt(x, y)\n",
        "        self.U -= learning_rate * dU\n",
        "        self.V -= learning_rate * dV\n",
        "        self.W -= learning_rate * dW\n",
        "\n",
        "    def train(self, X, Y, learning_rate=0.005, nepoch=100, evaluate_loss_after=5):\n",
        "        num_examples_seen = 0\n",
        "        losses = []\n",
        "        for epoch in range(nepoch):\n",
        "            if (epoch % evaluate_loss_after == 0):\n",
        "                loss = self.calculate_total_loss(X, Y)\n",
        "                losses.append((num_examples_seen, loss))\n",
        "                time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "                print(\"%s: Loss after num_examples_seen=%d epoch=%d: %f\" % (time, num_examples_seen, epoch, loss))\n",
        "                # Adjust the learning rate if loss increases\n",
        "                if len(losses) > 1 and losses[-1][1] > losses[-2][1]:\n",
        "                    learning_rate = learning_rate * 0.5\n",
        "                    print(\"Setting learning rate to %f\" % learning_rate)\n",
        "                sys.stdout.flush()\n",
        "            # For each training example...\n",
        "            for i in range(len(Y)):\n",
        "                self.sgd_step(X[i], Y[i], learning_rate)\n",
        "                num_examples_seen += 1\n",
        "        return losses"
      ],
      "metadata": {
        "id": "YS_Pf3uJqGAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mulGate = MultiplyGate()\n",
        "addGate = AddGate()\n",
        "activation = Tanh()"
      ],
      "metadata": {
        "id": "ntiFJAQ6mf70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "word_dim = 8000\n",
        "hidden_dim = 100\n",
        "X_train, y_train = getSentenceData('reddit.csv', word_dim)\n",
        "\n",
        "np.random.seed(10)\n",
        "rnn = Model(word_dim, hidden_dim)\n",
        "\n",
        "losses = rnn.train(X_train[:100], y_train[:100], learning_rate=0.005, nepoch=10, evaluate_loss_after=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2HOqJdhikLf",
        "outputId": "69c5e81e-eb95-49e3-a1e0-e8f7cf8729be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading CSV file...\n",
            "Parsed 79062 sentences.\n",
            "Found 63006 unique words tokens.\n",
            "Using vocabulary size 8000.\n",
            "The least frequent word in our vocabulary is 'appointments' and appeared 10 times.\n",
            "\n",
            "Example sentence: 'SENTENCE_START i joined a new league this year and they have different scoring rules than i'm used to. SENTENCE_END'\n",
            "\n",
            "Example sentence after Pre-processing: '['SENTENCE_START', 'i', 'joined', 'a', 'new', 'league', 'this', 'year', 'and', 'they', 'have', 'different', 'scoring', 'rules', 'than', 'i', \"'m\", 'used', 'to', '.', 'SENTENCE_END']'\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:49: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (78489,)\n",
            "y_train shape: (78489,)\n",
            "x:\n",
            "SENTENCE_START what are n't you understanding about this ? !\n",
            "[0, 52, 28, 17, 10, 858, 55, 26, 35, 70]\n",
            "\n",
            "y:\n",
            "what are n't you understanding about this ? ! SENTENCE_END\n",
            "[52, 28, 17, 10, 858, 55, 26, 35, 70, 1]\n",
            "2022-11-09 22:54:51: Loss after num_examples_seen=0 epoch=0: 8.987482\n",
            "2022-11-09 22:55:54: Loss after num_examples_seen=100 epoch=1: 8.973095\n",
            "2022-11-09 22:56:57: Loss after num_examples_seen=200 epoch=2: 8.951278\n",
            "2022-11-09 22:58:06: Loss after num_examples_seen=300 epoch=3: 8.908663\n",
            "2022-11-09 22:59:21: Loss after num_examples_seen=400 epoch=4: 8.810082\n",
            "2022-11-09 23:00:37: Loss after num_examples_seen=500 epoch=5: 6.947560\n",
            "2022-11-09 23:01:41: Loss after num_examples_seen=600 epoch=6: 6.303379\n",
            "2022-11-09 23:02:57: Loss after num_examples_seen=700 epoch=7: 5.993970\n",
            "2022-11-09 23:04:15: Loss after num_examples_seen=800 epoch=8: 5.795992\n",
            "2022-11-09 23:05:37: Loss after num_examples_seen=900 epoch=9: 5.664663\n"
          ]
        }
      ]
    }
  ]
}